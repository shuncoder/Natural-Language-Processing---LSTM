{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14770e88",
   "metadata": {},
   "source": [
    "# Text Classification with LSTM\n",
    "\n",
    "Phân loại văn bản tiếng Việt sử dụng LSTM.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded1177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt thư viện cần thiết\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Import các module cần thiết\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Import các module từ src\n",
    "from src.preprocess import preprocess_df\n",
    "from src.evaluate import evaluate_model, plot_confusion_matrix, save_evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66811225",
   "metadata": {},
   "source": [
    "## Load và tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85226428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc dữ liệu đã crawl\n",
    "df = pd.read_csv('data/dataset.csv')\n",
    "print(\"Kích thước dataset:\", df.shape)\n",
    "print(\"\\nPhân bố nhãn:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Tiền xử lý văn bản\n",
    "X, y = preprocess_df(df, text_col='summary', label_col='label')\n",
    "\n",
    "# Encode nhãn\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "labels = le.classes_\n",
    "\n",
    "# Chia train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nSố lượng classes:\", len(labels))\n",
    "print(\"Các nhãn:\", labels)\n",
    "print(\"\\nKích thước tập train/test:\", len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bfb6ac",
   "metadata": {},
   "source": [
    "## Chuẩn bị dữ liệu cho LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe5c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def yield_tokens(texts):\n",
    "    for text in texts:\n",
    "        yield text.split()\n",
    "\n",
    "# Xây dựng vocabulary\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(X_train), \n",
    "    specials=['<unk>', '<pad>'],\n",
    "    min_freq=2\n",
    ")\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def text_pipeline(text):\n",
    "    \"\"\"Convert text to tensor of indices.\"\"\"\n",
    "    return torch.tensor([vocab[token] for token in text.split()])\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return text_pipeline(self.texts[idx]), self.labels[idx]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"Pad sequences in batch to same length.\"\"\"\n",
    "    text_list, label_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        text_list.append(_text)\n",
    "        label_list.append(_label)\n",
    "    return (\n",
    "        pad_sequence(text_list, batch_first=True, padding_value=vocab['<pad>']),\n",
    "        torch.tensor(label_list)\n",
    "    )\n",
    "\n",
    "# Tạo DataLoader\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsDataset(X_train, y_train)\n",
    "test_dataset = NewsDataset(X_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "print(\"Kích thước vocab:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f93eacb",
   "metadata": {},
   "source": [
    "## Định nghĩa model LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7726bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, \n",
    "                 num_layers=2, bidirectional=True, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab['<pad>'])\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        lstm_out_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(lstm_out_dim, num_classes)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text shape: [batch_size, seq_len]\n",
    "        \n",
    "        # Get embeddings\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded shape: [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # lstm_out shape: [batch_size, seq_len, hidden_dim * num_directions]\n",
    "        \n",
    "        # Get final hidden state\n",
    "        hidden = lstm_out[:, -1, :]\n",
    "        # hidden shape: [batch_size, hidden_dim * num_directions]\n",
    "        \n",
    "        # Pass through linear layer\n",
    "        return self.fc(self.dropout(hidden))\n",
    "\n",
    "# Khởi tạo model\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "model = LSTMClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_classes=len(labels),\n",
    "    num_layers=2,\n",
    "    bidirectional=True,\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "# Loss và optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4d4b93",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe7923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc='Training', leave=False):\n",
    "        texts, labels = batch\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            texts, labels = batch\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    return (\n",
    "        total_loss / len(dataloader),\n",
    "        np.array(all_preds),\n",
    "        np.array(all_labels)\n",
    "    )\n",
    "\n",
    "# Training loop\n",
    "NUM_EPOCHS = 10\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_preds, val_labels = evaluate_epoch(\n",
    "        model, test_dataloader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}:\")\n",
    "    print(f\"Train loss: {train_loss:.4f}\")\n",
    "    print(f\"Val loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate and plot metrics\n",
    "    evaluate_model(\n",
    "        lambda x: val_preds,  # Dummy model that returns predictions\n",
    "        val_labels,\n",
    "        val_labels,\n",
    "        labels=labels\n",
    "    )\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'models/lstm_classifier.pt')\n",
    "        print(\"Saved best model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c7034",
   "metadata": {},
   "source": [
    "## Đánh giá model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b571c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model = LSTMClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_classes=len(labels),\n",
    "    num_layers=2,\n",
    "    bidirectional=True,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "best_model.load_state_dict(torch.load('models/lstm_classifier.pt'))\n",
    "\n",
    "# Get predictions\n",
    "_, test_preds, test_labels = evaluate_epoch(\n",
    "    best_model, test_dataloader, criterion, device\n",
    ")\n",
    "\n",
    "# Evaluate and save results\n",
    "save_evaluation_results(\n",
    "    test_labels,\n",
    "    test_preds,\n",
    "    labels=labels,\n",
    "    save_dir='results'\n",
    ")\n",
    "\n",
    "# Show confusion matrix\n",
    "plot_confusion_matrix(test_labels, test_preds, labels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b2ea6f",
   "metadata": {},
   "source": [
    "## Thử nghiệm với văn bản mới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786c3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text, model, vocab, le, device):\n",
    "    \"\"\"Dự đoán nhãn cho văn bản mới.\"\"\"\n",
    "    # Tiền xử lý\n",
    "    from src.preprocess import clean_text\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Chuyển thành tensor\n",
    "    with torch.no_grad():\n",
    "        inputs = text_pipeline(text).unsqueeze(0).to(device)\n",
    "        outputs = model(inputs)\n",
    "        pred = outputs.argmax(dim=1).item()\n",
    "    \n",
    "    return le.inverse_transform([pred])[0]\n",
    "\n",
    "# Thử nghiệm\n",
    "test_texts = [\n",
    "    \"Chính phủ họp bàn về tình hình kinh tế xã hội\",\n",
    "    \"Đội tuyển Việt Nam giành chiến thắng\",\n",
    "    \"iPhone 15 ra mắt với nhiều tính năng mới\",\n",
    "]\n",
    "\n",
    "print(\"\\nThử nghiệm dự đoán:\")\n",
    "for text in test_texts:\n",
    "    pred = predict_text(text, best_model, vocab, le, device)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Dự đoán: {pred}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
